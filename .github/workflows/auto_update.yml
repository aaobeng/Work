name: Ghana & Sports Mega-Sync

on:
  schedule:
    - cron: '*/10 * * * *' # every 10 minutes
  workflow_dispatch:

jobs:
  fetch-and-save:
    runs-on: ubuntu-latest
    permissions:
      contents: write

    steps:
      - name: Checkout Code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
          token: ${{ secrets.GITHUB_TOKEN }}

      - name: Fetch All Feeds
        shell: bash
        run: |
          echo "Creating public directory..."
          mkdir -p public

          UA="Mozilla/5.0"

          # --- GHANA NEWS ---
          curl -sL -A "$UA" "https://www.ghanaweb.com/rss/home.php" -o public/gh_ghanaweb.xml
          curl -sL -A "$UA" "https://www.graphic.com.gh/rss.xml" -o public/gh_graphic.xml
          curl -sL -A "$UA" "https://www.pulse.com.gh/feeds/news" -o public/gh_pulse.xml
          curl -sL -A "$UA" "https://citinewsroom.com/feed/" -o public/gh_citi.xml
          curl -sL -A "$UA" "https://www.myjoyonline.com/feed/" -o public/gh_joy.xml

          # --- GHANA FOOTBALL ---
          curl -sL -A "$UA" "https://ghanasoccernet.com/feed" -o public/gh_footy.xml
          curl -sL -A "$UA" "https://www.myjoyonline.com/sports/feed/" -o public/gh_joy_sports.xml
          curl -sL -A "$UA" "https://www.modernghana.com/sports/rss.xml" -o public/gh_modern_sports.xml
          curl -sL -A "$UA" "https://footballmadeinghana.com/feed/" -o public/gh_fm_football.xml

          # --- INTERNATIONAL FOOTBALL ---
          curl -sL -A "$UA" "https://www.skysports.com/rss/12040" -o public/intl_skyfootball.xml
          curl -sL -A "$UA" "https://www.fifa.com/rss/index.xml" -o public/intl_fifa.xml
          curl -sL -A "$UA" "https://www.espn.com/espn/rss/soccer/news" -o public/intl_espn.xml
          curl -sL -A "$UA" "http://feeds.bbci.co.uk/sport/football/rss.xml" -o public/intl_bbcfootball.xml

          # --- ADDITIONAL INTERNATIONAL LEAGUES ---
          curl -sL -A "$UA" "https://www.premierleague.com/rss/news.xml" -o public/intl_premierleague.xml
          curl -sL -A "$UA" "https://www.laliga.com/en-GB/rss" -o public/intl_laliga.xml
          curl -sL -A "$UA" "https://www.bundesliga.com/en/bundesliga/news/feed" -o public/intl_bundesliga.xml
          curl -sL -A "$UA" "https://www.legaseriea.it/en/feeds/news.xml" -o public/intl_seriea.xml
          curl -sL -A "$UA" "https://www.ligue1.com/rss" -o public/intl_ligue1.xml
          curl -sL -A "$UA" "https://www.uefa.com/rssfeed/uefachampionsleague/" -o public/intl_uefachampions.xml
          curl -sL -A "$UA" "https://www.uefa.com/rssfeed/uefaeuropaleague/" -o public/intl_uefaeuropa.xml
          curl -sL -A "$UA" "https://www.mlssoccer.com/rss" -o public/intl_mls.xml
          curl -sL -A "$UA" "https://www.cbf.com.br/rss" -o public/intl_brasileirao.xml

          # --- OTHER SPORTS ---
          curl -sL -A "$UA" "https://www.skysports.com/rss/12433" -o public/sports_f1.xml
          curl -sL -A "$UA" "https://www.skysports.com/rss/12110" -o public/sports_tennis.xml
          curl -sL -A "$UA" "https://www.espn.com/espn/rss/nba/news" -o public/sports_basketball.xml
          curl -sL -A "$UA" "https://www.skysports.com/rss/12027" -o public/sports_cricket.xml
          curl -sL -A "$UA" "https://www.eurosport.com/rss.xml" -o public/sports_mixed.xml

          # --- INTERNATIONAL NEWS ---
          curl -sL -A "$UA" "http://rss.cnn.com/rss/edition.rss" -o public/intl_cnn.xml
          curl -sL -A "$UA" "http://feeds.bbci.co.uk/news/world/rss.xml" -o public/intl_bbc.xml
          curl -sL -A "$UA" "http://feeds.reuters.com/reuters/topNews" -o public/intl_reuters.xml
          curl -sL -A "$UA" "https://www.aljazeera.com/xml/rss/all.xml" -o public/intl_aljazeera.xml

          # --- PROCESS AND FILTER LAST 24H ---
          python3 - <<'PYTHON_EOF'
            import xml.etree.ElementTree as ET
            import os
            from datetime import datetime, timedelta
            from email.utils import parsedate_to_datetime

            RETENTION_HOURS = 24

            def parse_items(path):
                if not os.path.exists(path) or os.path.getsize(path) < 500:
                    return []
                try:
                    root = ET.parse(path).getroot()
                    return root.findall('.//item')
                except:
                    return []

            def load_existing(file):
                if not os.path.exists(file):
                    return []
                try:
                    root = ET.parse(file).getroot()
                    return root.findall('.//item')
                except:
                    return []

            def is_recent(item):
                pub = item.find('pubDate')
                if pub is None:
                    return False
                try:
                    pub_date = parsedate_to_datetime(pub.text)
                    return datetime.utcnow() - pub_date <= timedelta(hours=RETENTION_HOURS)
                except:
                    return False

            def save_filtered(sources, output):
                existing = load_existing(output)
                all_items = []
                seen_links = set()

                for source in sources:
                    all_items.extend(source)
                all_items.extend(existing)

                filtered = []
                for item in all_items:
                    link = item.find('link')
                    if link is None:
                        continue
                    if link.text in seen_links:
                        continue
                    if not is_recent(item):
                        continue
                    seen_links.add(link.text)
                    filtered.append(item)

                root = ET.Element('rss', version='2.0')
                channel = ET.SubElement(root, 'channel')
                ET.SubElement(channel, 'title').text = "MegaSync"

                for item in filtered:
                    channel.append(item)

                ET.ElementTree(root).write(output, encoding='utf-8', xml_declaration=True)

            # --- Prepare sources ---
            gh_news_sources = [parse_items(f) for f in [
                "public/gh_ghanaweb.xml","public/gh_graphic.xml","public/gh_pulse.xml","public/gh_citi.xml","public/gh_joy.xml"
            ]]
            gh_football_sources = [parse_items(f) for f in [
                "public/gh_footy.xml","public/gh_joy_sports.xml","public/gh_modern_sports.xml","public/gh_fm_football.xml"
            ]]
            intl_news_sources = [parse_items(f) for f in [
                "public/intl_cnn.xml","public/intl_bbc.xml","public/intl_reuters.xml","public/intl_aljazeera.xml"
            ]]
            intl_football_sources = [parse_items(f) for f in [
                "public/intl_skyfootball.xml","public/intl_fifa.xml","public/intl_espn.xml","public/intl_bbcfootball.xml",
                "public/intl_premierleague.xml","public/intl_laliga.xml","public/intl_bundesliga.xml","public/intl_seriea.xml",
                "public/intl_ligue1.xml","public/intl_uefachampions.xml","public/intl_uefaeuropa.xml",
                "public/intl_mls.xml","public/intl_brasileirao.xml"
            ]]
            other_sports_sources = [parse_items(f) for f in [
                "public/sports_f1.xml","public/sports_tennis.xml","public/sports_basketball.xml",
                "public/sports_cricket.xml","public/sports_mixed.xml"
            ]]

            # --- Save each category ---
            save_filtered(gh_news_sources + intl_news_sources, "public/news.xml")
            save_filtered(gh_football_sources + intl_football_sources, "public/football.xml")
            save_filtered(other_sports_sources, "public/other_sports.xml")


      - name: Update status.json
        run: |
          echo "{\"lastUpdated\": \"$(date -u +%Y-%m-%dT%H:%M:%SZ)\"}" > public/status.json

      - name: Commit Changes
        run: |
          git config --global user.name "SkyBot"
          git config --global user.email "actions@github.com"
          git add public/
          git commit -m "üå™Ô∏è SkyBot: All Sports + News Sync ($(date -u +%H:%M) UTC)" || echo "No changes"
          git push
